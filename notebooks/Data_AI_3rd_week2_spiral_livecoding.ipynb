{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdcfzfnrozZMY26DEBvi/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jdasam/mas1004-2022/blob/main/notebooks/Data_AI_3rd_week2_spiral_livecoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9O-EojgkoEZC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from math import pi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if you run this code in your local computer, you have to check that torch is installed\n",
        "!pip install torch\n",
        "# But in Google Colab, torch is already installed "
      ],
      "metadata": {
        "id": "bomZsxwfoNGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make spiral data\n",
        "# https://gist.github.com/45deg/e731d9e7f478de134def5668324c44c5\n",
        "N = 500\n",
        "theta = np.sqrt(np.random.rand(N))*2*pi # np.linspace(0,2*pi,100)\n",
        "\n",
        "r_a = 2*theta + pi\n",
        "data_a = np.array([np.cos(theta)*r_a, np.sin(theta)*r_a]).T\n",
        "x_a = data_a + np.random.randn(N,2)\n",
        "\n",
        "r_b = -2*theta - pi\n",
        "data_b = np.array([np.cos(theta)*r_b, np.sin(theta)*r_b]).T\n",
        "x_b = data_b + np.random.randn(N,2)\n",
        "\n",
        "res_a = np.append(x_a, np.zeros((N,1)), axis=1)\n",
        "res_b = np.append(x_b, np.ones((N,1)), axis=1)\n",
        "\n",
        "res = np.append(res_a, res_b, axis=0)\n",
        "np.random.shuffle(res)"
      ],
      "metadata": {
        "id": "wA2xUxDsoLzE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def visualize_decision_boundary(model, data, label):\n",
        "  x_linspace = torch.linspace(min(data[:,0]), max(data[:,0]), steps=200)\n",
        "  y_linspace = torch.linspace(min(data[:,1]), max(data[:,1]), steps=200)\n",
        "  grid_x, grid_y = torch.meshgrid(x_linspace, y_linspace)\n",
        "  grid_xy = torch.stack([grid_x, grid_y]).permute(1,2,0)\n",
        "  grid_xy = grid_xy.view(-1, 2)\n",
        "  if isinstance(model, torch.nn.Module):\n",
        "    value_by_grid = model(grid_xy)\n",
        "  else:\n",
        "    value_by_grid = run_neuron(model, grid_xy)\n",
        "  value_by_grid = value_by_grid.view(200, 200, 1)\n",
        "  value_by_grid[value_by_grid<=0.5] = 0\n",
        "  value_by_grid[value_by_grid>0.5] = 1\n",
        "\n",
        "  plt.scatter(x=data[label[:,0]==0,0], y=data[label[:,0]==0,1])\n",
        "  plt.scatter(x=data[label[:,0]==1,0], y=data[label[:,0]==1,1])\n",
        "\n",
        "  plt.contourf(grid_x.detach().numpy(), grid_y.detach().numpy(), value_by_grid.detach().numpy().squeeze(), alpha=0.3)"
      ],
      "metadata": {
        "id": "Z-o1l0qPbMZb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's convert our data in tensor\n",
        "datas = torch.tensor(res, dtype=torch.float32)\n",
        "# represent each number with 32 bits, instead of 64 bits"
      ],
      "metadata": {
        "id": "7E-Zx8HBjh4A"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datas # a tensor can only have one data type.\n",
        "# so if you want to represent float data\n",
        "# you have to represent even integer in float"
      ],
      "metadata": {
        "id": "SCeh8daDjroy",
        "outputId": "13173ae8-f408-444f-c95c-ecd0f42c76de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -9.7528,   1.3513,   0.0000],\n",
              "        [ 13.8056,  -4.6267,   0.0000],\n",
              "        [ 11.5181,  -9.6006,   0.0000],\n",
              "        ...,\n",
              "        [-15.1413,   0.9364,   1.0000],\n",
              "        [  8.6965, -10.3705,   0.0000],\n",
              "        [ 17.1500,  -0.6040,   0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datas[datas[:,-1]==0]"
      ],
      "metadata": {
        "id": "WBfdHqHgkD1d",
        "outputId": "0fb0111b-e8f2-4e0f-a8d0-1cec3f606c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -9.7528,   1.3513,   0.0000],\n",
              "        [ 13.8056,  -4.6267,   0.0000],\n",
              "        [ 11.5181,  -9.6006,   0.0000],\n",
              "        ...,\n",
              "        [ -7.4159, -11.0369,   0.0000],\n",
              "        [  8.6965, -10.3705,   0.0000],\n",
              "        [ 17.1500,  -0.6040,   0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_xy = datas[:, :2]\n",
        "data_label= datas[:, -1]"
      ],
      "metadata": {
        "id": "vnUXwvPFlUcD"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_xy, data_label[:10]"
      ],
      "metadata": {
        "id": "ANeki4J5lap4",
        "outputId": "95223394-0719-47ba-b89d-a4197131f481",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ -9.7528,   1.3513],\n",
              "         [ 13.8056,  -4.6267],\n",
              "         [ 11.5181,  -9.6006],\n",
              "         ...,\n",
              "         [-15.1413,   0.9364],\n",
              "         [  8.6965, -10.3705],\n",
              "         [ 17.1500,  -0.6040]]),\n",
              " tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 0.]))"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# How to make a tensor in specific shape\n",
        "# Let's make 2 x 3 tensor\n",
        "print(torch.zeros(2,3))\n",
        "print(torch.ones(2,3))\n",
        "print(torch.rand(2,3)) # every element will have random value between 0-1\n",
        "print(torch.randn(2,3)) # this uses gaussian (normal) distribution\n",
        " "
      ],
      "metadata": {
        "id": "1E_hxsbJmPsj",
        "outputId": "9a9967e0-f978-4e99-fa69-b121474d7c55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0.0152, 0.9159, 0.7349],\n",
            "        [0.6722, 0.1061, 0.5764]])\n",
            "tensor([[ 0.3416, -0.9599, -2.6229],\n",
            "        [-0.3976,  0.0559,  1.7811]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make a neural network layer\n",
        "\n",
        "\n",
        "class Layer:\n",
        "  def __init__(self, in_features, num_neurons):\n",
        "    # in_features: how many features does this layer take as an input\n",
        "    # \n",
        "    self.in_features = in_features\n",
        "    self.num_neurons = num_neurons\n",
        "\n",
        "    # next step: define a weight matrix\n",
        "    self.weight = torch.randn(self.in_features, self.num_neurons)\n",
        "  \n",
        "  def __call__(self, input):\n",
        "    return torch.mm(input, self.weight)\n",
        "\n",
        "layer = Layer(2, 4)\n",
        "layer.weight"
      ],
      "metadata": {
        "id": "HUf4zt2QkM17",
        "outputId": "75a00129-28ba-4216-cf6f-96036021386e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3099, -0.1747,  0.5885, -0.1636],\n",
              "        [ 0.6864,  1.9186, -0.8930, -0.1188]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_xy.shape, layer.weight.shape"
      ],
      "metadata": {
        "id": "uF5T5n9nnwRA",
        "outputId": "05675e32-0d80-4d9c-f31f-e4472c586440",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1000, 2]), torch.Size([2, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what will be the output's size\n",
        "output = torch.zeros(data_xy.shape[0], layer.weight.shape[1])\n",
        "\n",
        "for data_idx, data_sample in enumerate(data_xy):\n",
        "  for neuron_idx in range(layer.weight.shape[1]):\n",
        "    temporary_sum = 0\n",
        "    for feature_idx in range(len(data_sample)):\n",
        "      data_pos = data_sample[feature_idx]\n",
        "      corresp_neuron_weight = layer.weight[feature_idx, neuron_idx]\n",
        "      temporary_sum += data_pos * corresp_neuron_weight\n",
        "    # print(data_sample[i])\n",
        "    output[data_idx, neuron_idx] = temporary_sum "
      ],
      "metadata": {
        "id": "CfzA-yo1n6ep"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output"
      ],
      "metadata": {
        "id": "IuyUNqsvqPH8",
        "outputId": "63a37cdb-b4be-4afc-dbd2-dd7c3a041e81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.3134,  -3.3787,   6.2090,   9.5663],\n",
              "        [  1.0399,   0.5123,  -9.5956, -19.9656],\n",
              "        [  4.0055,  -8.6053,  -9.7113, -30.2453],\n",
              "        ...,\n",
              "        [  1.1214,  -7.0731,   9.2944,  12.1026],\n",
              "        [  4.7307, -11.4095,  -8.2598, -30.2254],\n",
              "        [ -1.5197,   8.7299, -10.3918, -12.6274]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Easier, and more efficient way\n",
        "torch.mm(data_xy, layer.weight) # mm means matrix multiplication"
      ],
      "metadata": {
        "id": "QOaln0x1qSr6",
        "outputId": "71ffca8f-9289-414a-aec9-7931c485c91d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.3134,  -3.3787,   6.2090,   9.5663],\n",
              "        [  1.0399,   0.5123,  -9.5956, -19.9656],\n",
              "        [  4.0055,  -8.6053,  -9.7113, -30.2453],\n",
              "        ...,\n",
              "        [  1.1214,  -7.0731,   9.2944,  12.1026],\n",
              "        [  4.7307, -11.4095,  -8.2598, -30.2254],\n",
              "        [ -1.5197,   8.7299, -10.3918, -12.6274]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# computation time with for loop vs matrix multiplication\n",
        "import time\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "output = torch.zeros(data_xy.shape[0], layer.weight.shape[1])\n",
        "for data_idx, data_sample in enumerate(data_xy):\n",
        "  for neuron_idx in range(layer.weight.shape[1]):\n",
        "    temporary_sum = 0\n",
        "    for feature_idx in range(len(data_sample)):\n",
        "      data_pos = data_sample[feature_idx]\n",
        "      corresp_neuron_weight = layer.weight[feature_idx, neuron_idx]\n",
        "      temporary_sum += data_pos * corresp_neuron_weight\n",
        "    # print(data_sample[i])\n",
        "    output[data_idx, neuron_idx] = temporary_sum\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"time spent: \", end_time - start_time) "
      ],
      "metadata": {
        "id": "icmgn9ntqzYu",
        "outputId": "819cc623-19dc-4b3d-9605-2a7d2dc42569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time spent:  0.48915719985961914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Never tries to use for loop in matrix multiplication\n",
        "# matrix multiplication is much much faster than for loop\n",
        "\n",
        "start_time = time.time()\n",
        "torch.mm(data_xy, layer.weight)\n",
        "end_time = time.time()\n",
        "print(end_time - start_time)"
      ],
      "metadata": {
        "id": "rrw_jU9XrSOf",
        "outputId": "2a2c593d-7f13-4ea9-82e3-ee8568a04379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.009114742279052734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(x):\n",
        "  # x is an tensor\n",
        "  is_larger_than_zero = x > 0\n",
        "  new_x = torch.clone(x)\n",
        "  new_x[~is_larger_than_zero] = 0\n",
        "  # print(is_larger_than_zero.shape, x.shape)\n",
        "  # the index of tensor can have the same shape with the tensor itself\n",
        "  return new_x\n",
        "relu(output)"
      ],
      "metadata": {
        "id": "z5cdTkEbs5vc",
        "outputId": "2557e361-ce5a-4c04-9544-080d3e005a62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3134,  0.0000,  6.2090,  9.5663],\n",
              "        [ 1.0399,  0.5123,  0.0000,  0.0000],\n",
              "        [ 4.0055,  0.0000,  0.0000,  0.0000],\n",
              "        ...,\n",
              "        [ 1.1214,  0.0000,  9.2944, 12.1026],\n",
              "        [ 4.7307,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  8.7299,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output > 0\n"
      ],
      "metadata": {
        "id": "coYA4qWhtHSE",
        "outputId": "aab28743-b167-4f6d-e5e9-95c100821c92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True, False,  True,  True],\n",
              "        [ True,  True, False, False],\n",
              "        [ True, False, False, False],\n",
              "        ...,\n",
              "        [ True, False,  True,  True],\n",
              "        [ True, False, False, False],\n",
              "        [False,  True, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "layer_0 = Layer(in_features=2, num_neurons=4)\n",
        "layer_1 = Layer(in_features=4, num_neurons=1)\n",
        "\n",
        "print(data_xy.shape)\n",
        "out_0 = layer_0(data_xy)\n",
        "print(out_0.shape)\n",
        "out_1 = layer_1(relu(out_0))\n",
        "print(out_1.shape)"
      ],
      "metadata": {
        "id": "cdLoc1TUrtSY",
        "outputId": "2d8098c4-452a-4fe6-b38f-a82a8b308c05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1000, 2])\n",
            "torch.Size([1000, 4])\n",
            "torch.Size([1000, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scientific notation\n",
        "# 9.4111e-01 means 9.4111 * 10 ** (-1), 0.94111\n",
        "\n",
        "# turn off scientific notation\n",
        "torch.set_printoptions(sci_mode=False)"
      ],
      "metadata": {
        "id": "OiAobuHgsOnV"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out_1.shape"
      ],
      "metadata": {
        "id": "28bMH8fauN6d",
        "outputId": "d1fe2cbf-519f-47ff-98f5-a57e6517485d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flattening, or squeezing the tensor\n",
        "out_1[:, 0].shape # if you select only one index in a certain dimension,\n",
        "# it will delete that dimension \n"
      ],
      "metadata": {
        "id": "6GzXuxkDuaBn",
        "outputId": "01994e20-97f3-4adc-9f30-c93d2e8da8f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000])"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out_1.squeeze() # this deletes every dimension with size 1"
      ],
      "metadata": {
        "id": "i973KSBSulNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's suppose that out_1 is our model's prediction for a given input\n",
        "# out_1[n] is the prediction value of whether the n-th input data is category 1\n",
        "\n",
        "# our target value is data_label\n",
        "data_label"
      ],
      "metadata": {
        "id": "BtoZ1hk6ut5_",
        "outputId": "30b969e5-b63a-463f-be5d-f3423e9c735c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
              "        0., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
              "        0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
              "        0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
              "        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
              "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
              "        0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
              "        0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
              "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
              "        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
              "        0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
              "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
              "        0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
              "        1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
              "        0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
              "        1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
              "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
              "        0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
              "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
              "        1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
              "        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
              "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
              "        1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n",
              "        0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
              "        1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
              "        1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
              "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
              "        1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "        0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
              "        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
              "        1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
              "        0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1.,\n",
              "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
              "        1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
              "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0.,\n",
              "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
              "        0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
              "        1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
              "        1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
              "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
              "        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
              "        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
              "        1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "        1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
              "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define how bad our model is.\n",
        "pred = out_1.squeeze()\n",
        "\n",
        "pred[:10], data_label[:10]\n",
        "torch.abs(pred-data_label)"
      ],
      "metadata": {
        "id": "tJEK5kj-vFXV",
        "outputId": "cea99895-365a-45a9-95a2-1fd1e6f5e642",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([    5.2484,    18.3531,    27.2993,    13.5547,    24.9371,    15.4235,\n",
              "           31.1061,     1.9519,     4.5414,    26.1178,     6.3132,     4.3333,\n",
              "            3.3074,     3.0512,    19.4584,    18.2753,     3.8892,    13.9980,\n",
              "            5.3480,    10.0630,     1.2623,     6.4788,     6.9836,    12.1208,\n",
              "            7.9826,     6.1282,     8.4529,     7.1248,    21.8367,     1.3863,\n",
              "            2.5178,     5.7412,     2.5511,     4.1078,     8.9101,     7.1181,\n",
              "           29.6483,    16.9841,    15.0757,     2.3522,     0.1663,     0.7554,\n",
              "            4.3613,     0.2320,    12.2196,    19.0414,    27.8793,    14.5732,\n",
              "            0.1468,     1.8627,    17.8519,     2.5384,     3.4768,    14.7084,\n",
              "           20.0119,    19.3893,     2.1314,     3.5868,    25.0628,    14.4338,\n",
              "            7.4742,    26.5770,     7.6751,     5.6576,    21.8059,     3.8442,\n",
              "            9.5953,    12.1559,     4.1130,     7.4440,    14.5478,     4.9913,\n",
              "            3.8095,     6.8092,     1.5463,     4.3208,     2.1456,    14.7408,\n",
              "            0.5292,     2.5868,    12.6643,    30.4551,    10.5314,     1.0223,\n",
              "           15.2728,    11.9961,    26.7966,     3.8038,     6.3046,    10.5768,\n",
              "           19.0907,    13.0776,    27.9378,    27.8225,    14.7929,    12.7721,\n",
              "           22.2062,    16.1555,     2.5990,    26.9519,     2.8084,     7.1677,\n",
              "           22.5594,    11.8248,     8.2230,    18.7056,    17.3032,     3.4304,\n",
              "            5.7628,    15.8341,    16.6160,    11.1285,     2.8192,    18.4526,\n",
              "            6.9184,    14.1416,    16.8867,     4.8125,    23.2112,     1.2111,\n",
              "           14.1208,    24.0360,    12.3061,    16.2786,     1.0785,     0.9553,\n",
              "           30.5997,     7.2663,    16.7141,    16.6176,     2.9355,    27.4977,\n",
              "           20.5183,    25.6179,     6.3703,    23.3054,    13.3785,     4.0570,\n",
              "            8.3135,     6.2090,     9.9024,     4.2891,    14.5367,     5.6434,\n",
              "            5.8563,    24.4556,    16.6164,     3.8554,     2.0306,     3.6923,\n",
              "           26.3782,     7.3037,    16.8333,    10.9927,     1.5755,    23.6850,\n",
              "            6.1274,     7.8886,     4.7671,    27.2901,     1.6278,    11.5199,\n",
              "            0.8182,     2.2323,     8.0694,    27.0879,     7.7757,     4.1994,\n",
              "           27.2138,     5.7427,    29.0564,     7.8181,    17.3300,    23.8956,\n",
              "           19.6858,     4.2845,     7.8397,     0.2209,     3.5766,     3.8306,\n",
              "            5.6613,    30.5232,     1.5363,    14.6010,     3.5341,     3.0177,\n",
              "            1.6688,    23.7320,    13.5036,     2.5540,     5.0007,     4.3158,\n",
              "           14.0275,     2.3560,    15.3208,    13.0273,     3.0447,    10.4037,\n",
              "            1.7901,     1.7321,     1.6123,     6.4133,     4.5569,     7.1972,\n",
              "           28.0667,    28.6103,     2.5750,    29.0594,     0.3998,     0.7256,\n",
              "            2.0710,    22.8571,    13.8033,     7.1410,    25.7513,     3.0094,\n",
              "           25.2733,    26.5690,     6.3767,     3.0091,    15.7003,     7.3989,\n",
              "            5.0804,    29.1467,     7.1417,     2.7291,     6.5338,    14.6160,\n",
              "           15.2094,     2.5535,     2.2580,     7.4196,    15.1813,     3.2460,\n",
              "            7.7549,     4.3502,     2.2046,    19.8587,    22.6871,     2.6002,\n",
              "            5.7172,    25.6820,     0.6560,    15.0543,     7.0078,    11.6088,\n",
              "           28.8862,    12.8675,    26.4443,    14.7597,     0.7076,     2.9710,\n",
              "            0.3701,     2.3571,     6.5166,    24.3431,    15.6990,     7.3566,\n",
              "           27.3802,     6.2728,     5.3267,     1.1243,     7.5828,     5.0815,\n",
              "           13.4955,    15.7297,     7.2359,     0.3556,    17.8081,    19.6142,\n",
              "           26.0486,     7.1959,    14.1890,     5.4879,    16.8465,     0.7548,\n",
              "            6.7238,     7.9306,     4.2654,    27.1999,    26.5770,    23.8111,\n",
              "            2.8192,     4.9882,     7.3591,    13.8845,     4.9101,     2.1996,\n",
              "            6.7917,     1.8496,     4.5356,     0.9663,    12.6800,    12.2510,\n",
              "            0.2901,     2.7670,    15.0196,     5.0794,     0.2622,     7.4156,\n",
              "           19.7960,     7.1166,     2.5886,    30.9133,     2.5347,     2.5618,\n",
              "            7.2967,    23.5991,    30.4848,     2.3504,    22.3190,    25.5000,\n",
              "            0.0546,     0.7892,     7.0278,    11.9404,    15.0654,    31.5174,\n",
              "           26.8325,     0.5312,     6.3979,    10.9321,    15.8895,     3.0502,\n",
              "           31.1888,     0.5559,     4.4249,     3.0780,     6.6806,    13.5171,\n",
              "           18.4769,    16.6243,     6.9635,     2.1698,    30.0466,     1.6986,\n",
              "           21.2670,    12.9100,     6.8240,     2.8850,     8.0805,     3.1989,\n",
              "            4.5367,    21.9544,    25.1691,     4.9089,     3.9451,     1.1086,\n",
              "           22.1500,    16.0687,    27.1958,     3.6389,     4.8493,     0.4057,\n",
              "            0.2121,     0.4647,     3.9111,    28.0561,     0.2302,     6.1717,\n",
              "           15.7592,     3.8661,    27.9628,    19.3828,    14.5475,     3.7554,\n",
              "            0.8587,     6.3308,     0.3202,     1.5715,    25.3850,     5.9056,\n",
              "            4.5914,     4.8496,     0.8445,     6.8735,     2.3275,     6.0129,\n",
              "            4.9172,    19.7523,    16.4119,     7.5782,     3.3225,     7.7514,\n",
              "           29.6460,     1.8793,    10.4415,     2.1733,     4.1225,    25.4883,\n",
              "            0.3090,    10.5986,     8.9589,    30.8167,     5.8125,     6.3976,\n",
              "           28.7710,     7.4362,    26.8354,     7.3261,     0.1865,     2.7198,\n",
              "           16.0677,     4.3732,     6.6510,     0.3888,     0.5778,    29.5144,\n",
              "            1.2680,     4.7752,    28.6235,    14.2937,    25.7451,     0.3627,\n",
              "            1.1539,    21.3492,     1.8296,     0.1808,     7.1133,     8.2627,\n",
              "            0.7582,    15.4336,     5.2980,    24.6600,    32.2792,    22.0553,\n",
              "            0.8112,     5.4231,     1.3544,     0.9443,     3.8271,     0.0163,\n",
              "            0.9607,    24.3443,    15.5221,     5.2667,     2.8889,     4.2370,\n",
              "            0.3022,     4.4109,     4.4535,    26.2233,    21.6879,     2.6968,\n",
              "           14.4849,     0.2170,     6.4739,     7.2201,    12.1299,     4.3032,\n",
              "           15.4825,     2.2302,     0.8287,     4.0250,     4.8179,    18.6611,\n",
              "            4.4110,     7.8841,     0.1415,    11.0609,    25.8794,     2.9956,\n",
              "           25.3654,     5.2980,    17.4652,     7.1562,     0.5304,    23.3764,\n",
              "            4.3015,     0.3986,     6.5897,    25.8285,    11.6034,     6.6157,\n",
              "           18.2263,     3.7978,     1.7921,    28.7242,    11.1812,     2.0852,\n",
              "            6.5067,    21.7884,    28.2345,     0.7766,     8.7359,     6.9659,\n",
              "            0.6959,    18.2296,     0.6405,     2.7422,    11.7486,     6.4773,\n",
              "            5.0757,     4.1715,    15.0875,    27.0495,    28.6017,     5.0094,\n",
              "            7.4084,     2.9470,    13.7286,    20.7799,     7.3173,     4.9631,\n",
              "            4.4729,     6.6050,     2.5655,     4.8950,     0.4359,    16.4616,\n",
              "           28.7470,    22.2119,    15.0095,     2.0584,    14.0656,     0.8570,\n",
              "            6.0225,    29.3345,    32.2174,    29.1582,    10.0220,     0.4985,\n",
              "            7.5460,    26.5211,    15.9650,     0.9504,    17.2802,    25.2980,\n",
              "            2.1508,     7.0355,     3.5592,     6.4368,    11.7904,     4.1907,\n",
              "           31.8981,     0.6809,     6.1136,    15.6596,     4.6604,     2.2649,\n",
              "            2.7010,     4.3037,    25.3591,     6.5195,     7.0802,     7.1121,\n",
              "           12.5116,    22.9212,    29.1037,    20.7827,     3.1236,     2.6994,\n",
              "            2.9552,    15.9168,    25.0863,     7.4736,     2.4107,     0.5150,\n",
              "            7.5460,     0.4051,    30.2269,    13.7206,    29.5957,    24.5428,\n",
              "           12.4659,     4.8030,    12.7683,    31.5693,     1.6052,    26.3974,\n",
              "            4.1427,     7.9045,    18.1845,     7.3503,     4.4756,    18.8741,\n",
              "            2.0076,    23.4572,    11.3406,     7.3843,     2.6182,     3.8970,\n",
              "           13.3650,    14.7099,     0.5767,     7.7650,    25.1871,     4.8781,\n",
              "            2.9491,    18.4664,     4.2905,    24.9432,     4.4591,     3.1458,\n",
              "            4.2973,     0.0589,     8.8791,     7.4483,    29.0849,    22.2968,\n",
              "            1.2514,    15.1817,     0.7598,     0.9466,    26.6606,     6.3473,\n",
              "           15.4072,     2.0995,     0.4220,     2.7700,     1.9796,     2.3835,\n",
              "           25.6437,    26.7201,     6.9320,     5.9362,     6.9436,    11.0868,\n",
              "           29.1821,     0.6358,    25.0335,    21.6935,     6.8673,    18.4172,\n",
              "            6.5500,    14.9551,     7.1166,    25.6534,     0.7309,     7.5618,\n",
              "            4.3460,     0.6788,     4.9687,    12.7884,    23.6007,     3.0329,\n",
              "            9.0025,    11.5376,     4.4850,     2.5083,     4.5085,    19.3278,\n",
              "            0.6916,     0.1572,     1.6776,    20.2950,     3.0496,    22.3561,\n",
              "            1.9856,     3.9568,    11.6296,    24.8208,     2.0515,     7.2558,\n",
              "            9.2183,     1.1686,     3.6499,     7.4442,    14.4509,    31.0971,\n",
              "           23.0804,    15.4625,    18.4900,    16.5836,    25.4974,    19.1897,\n",
              "           23.3471,    16.9765,    17.7986,     7.7032,     5.1103,     0.2276,\n",
              "            2.3050,     2.3892,    31.6045,     5.6994,     1.1045,     7.2355,\n",
              "            0.8819,    29.8375,    15.7400,     1.6121,     2.5010,    17.5068,\n",
              "            6.4120,     5.3511,     2.1744,     4.2081,     7.7214,    15.8986,\n",
              "            0.4871,     1.1976,     1.0230,    19.9516,     2.8771,     0.8873,\n",
              "            4.2735,     2.3051,    19.7036,    27.0031,    16.5864,     0.8715,\n",
              "           20.0127,    10.8895,     4.0951,    28.1494,     5.9852,     2.5611,\n",
              "            7.5952,     3.8770,    25.7316,     0.2693,    22.5663,     0.8571,\n",
              "           22.7879,    13.0231,    27.2943,     3.8835,    19.6476,     7.0327,\n",
              "           20.9105,     4.1711,    29.2285,    12.7101,    26.5667,    25.6456,\n",
              "            5.7989,     7.4686,    27.8871,     6.8203,    13.0112,     2.8883,\n",
              "           26.4379,     0.4941,    14.5393,    13.4451,    19.2533,    11.4073,\n",
              "           18.7026,     4.9618,     7.5023,    23.8793,     8.5958,     2.8228,\n",
              "            0.3055,     5.9794,     4.9752,     6.0317,     5.4994,     0.3009,\n",
              "            5.7985,     4.6419,    21.5922,    11.0745,     2.8726,     5.4306,\n",
              "            7.2629,     4.5355,    20.4117,     6.8265,     4.1655,     1.0987,\n",
              "           26.0165,    10.9404,    27.7369,     7.5466,     8.2354,    26.9318,\n",
              "           13.4004,     8.9579,     2.3314,     2.4867,     6.0117,    12.1393,\n",
              "           14.8268,     0.5731,     2.7581,     8.4635,     2.1046,     3.5255,\n",
              "            7.7912,     3.5400,    19.8244,    15.0857,     4.0509,     6.6183,\n",
              "            0.3272,    28.1081,     0.3824,     1.6985,     6.1053,     4.1652,\n",
              "            2.9867,     2.7623,     3.9217,     3.2652,     3.6584,     8.9862,\n",
              "            3.7866,    15.3419,    26.9877,    19.2869,     7.0200,    17.3811,\n",
              "           21.7447,     0.5228,    13.5279,     3.2285,     7.6236,     0.2980,\n",
              "            1.4436,     4.8303,     0.6424,    15.2419,     2.2333,    13.5193,\n",
              "            6.5674,     1.5938,    18.2617,    24.1768,    15.5250,    13.5452,\n",
              "            4.6961,     0.8272,     3.0433,     3.8925,     5.7375,     7.2367,\n",
              "           30.9247,     6.7218,     8.1387,     3.1312,     4.6319,     4.2930,\n",
              "            2.8879,     3.5953,     6.7630,     0.2412,     1.6480,    17.2118,\n",
              "            5.5314,    11.7743,    15.8487,     6.7524,    20.5034,    13.9988,\n",
              "            3.6811,    28.7519,    11.7505,     7.4038,     5.1869,     9.5203,\n",
              "            7.5180,     4.8472,     0.7758,     6.1666,    25.3289,     7.0731,\n",
              "            0.8652,     7.7560,    22.8653,    14.6327,    12.1476,    17.1345,\n",
              "           12.6784,     3.3960,     9.6163,    11.6864,     6.9427,     0.0525,\n",
              "            2.7853,     7.7941,    25.3648,     2.0079,    21.2770,    16.8566,\n",
              "            2.7673,     0.6099,     2.9805,     6.8410,     3.5290,     1.5713,\n",
              "           29.1379,     4.4401,    32.0443,    17.6623,    17.4992,    27.6293,\n",
              "           28.8025,     0.2975,     4.3769,    14.5269,     9.3495,     4.5024,\n",
              "            3.9112,     0.1589,    29.4496,     8.6654,     4.3568,     3.8513,\n",
              "            1.7323,    25.7139,    31.4949,    22.3199,     3.9938,     0.5995,\n",
              "           29.5483,    23.0250,     4.6980,     1.2975,     4.9837,     2.0784,\n",
              "           11.9840,    13.6973,    18.5372,    21.8445,     7.4281,    21.2030,\n",
              "           11.6956,     4.2333,     4.9082,    27.0457,     4.2087,     9.2289,\n",
              "            0.9586,    16.3496,     7.3070,    23.5884,     0.1269,    16.8018,\n",
              "            0.6940,    23.1901,    10.5140,     5.1631,    25.9154,    14.6917,\n",
              "            5.7122,     5.6553,    32.2438,    11.3854,     2.5641,    25.8251,\n",
              "            0.5901,    14.3602,    10.1439,    19.5492,     7.7458,     3.3448,\n",
              "           27.9193,     2.3652,    19.3728,     4.9112,     5.4310,    14.9628,\n",
              "           29.1255,    24.5772,     5.2830,    28.0704,     5.6826,     2.0953,\n",
              "           11.2038,     1.9172,     4.4519,     5.7335,    18.6921,     6.1016,\n",
              "           22.3242,    16.9057,     2.2720,     4.2812,     4.9178,     6.6027,\n",
              "            2.8865,    10.8272,     3.2451,     6.3646,     3.4819,     1.6331,\n",
              "            0.2345,     4.6988,     0.9642,     0.7394,     1.4422,     4.4038,\n",
              "           22.4767,    30.4668,     4.4065,     6.6083,     7.6717,    15.3823,\n",
              "            5.1853,     6.9863,    21.4882,     3.5003,    21.0411,    28.0269,\n",
              "           17.9265,     6.8075,    27.1306,    12.8518])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    }
  ]
}